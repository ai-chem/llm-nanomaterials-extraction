# LLM4Nano: Evaluation of Large Language Models for Nanomaterials Data Extraction

## Abstact
The rise of large language models (LLMs) has sparked interest in automating information extraction from scientific literature. However, their capabilities in highly specialized and multimodal domains such as nanomaterials remain uncertain. This study evaluates the performance of four cutting-edge LLMs—Cloude 2.7 Sonnet, LLaMA 4, Mistral Small 3.1, and Qwen2.5-VL-72B—on structured data extraction from nanomaterials research articles. Using open-access datasets from the ChemX benchmark, we assess each model’s ability to identify key physicochemical and experimental parameters from unstructured text and figures. Our results reveal that current LLMs struggle with modality integration, numerical accuracy, and chemical entity resolution despite their general language understanding. In contrast, nanoMINER, a domain-specialized multi-agent system, consistently outperforms all general-purpose models across precision, recall, and entity completeness. These findings underscore the limitations of deploying generic LLMs for domain-specific extraction tasks and highlight the need for hybrid, multimodal, and task-adapted architectures in scientific information mining.


## Citation

Under review in EMNLP 2025
